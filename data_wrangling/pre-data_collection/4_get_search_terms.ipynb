{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the on-topic tweets from my high-volume users, I performed a series of basic processing steps to extract the most commonly used words and hashtags. These are the terms I used in my search to obtain my actual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import the things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import glob2\n",
    "import data\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "import en_core_web_trf\n",
    "import en_core_web_md\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load(\"en_core_web_md\") \n",
    "# nlp = en_core_web_trf.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get common words, hashtags, mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tweets saved in previous step\n",
    "fl = os.path.join(data.__path__[0], r\"raw\\high_count_user_tweets.json\")\n",
    "with open(fl, 'r') as f:\n",
    "    tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_str = \" \".join(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = ['rt']\n",
    "for word in custom_stop_words:\n",
    "    STOP_WORDS.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(tweet_str):\n",
    "    \"\"\"takes a 1d list of strings\n",
    "    uses some code from datacamp's NLP course\"\"\" \n",
    "    t_lower = tweet_str.lower().strip() #lowercase everything\n",
    "    hashtags = re.findall(r\"#(\\w+)\", t_lower) #pull out hashtags\n",
    "    mentions = re.findall(r\"@(\\w+)\", t_lower) #pull out mentions\n",
    "    t_words = [word for word in t_lower.split(' ') if word.isalpha()]\n",
    "    t_str =  \" \".join(t_words)\n",
    "\n",
    "#     [t_lower.remove(ht) for ht in hashtags]\n",
    "#     [t_lower.remove(mt) for mt in mentions]  \n",
    "    \n",
    "    t_doc = nlp(t_str) #create nlp object\n",
    "    t_lems = [t.lemma_ for t in t_doc] #get lemmas\n",
    "    t_clean = [t_lem for t_lem in t_lems if t_lem not in STOP_WORDS] #remove stop words, non-alpha words     \n",
    "\n",
    "    \n",
    "#     if custom_stops is not None:\n",
    "#         [t_clean.remove(stop) for stop in custom_stops if stop in t_clean]\n",
    "  \n",
    "    return t_doc, t_clean, hashtags, mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_doc, t_clean, hashtags, mentions = preprocess_tweets(tweets_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hiv', 77),\n",
       " ('plhiv', 16),\n",
       " ('endhivstigma', 14),\n",
       " ('celebrateblackwomen', 13),\n",
       " ('trans', 9)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_count = Counter(hashtags)\n",
    "h_count.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blackaids', 10),\n",
       " ('uspwn', 9),\n",
       " ('sexhistorian', 8),\n",
       " ('cdc_hivaids', 5),\n",
       " ('hspn4', 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_count = Counter(mentions)\n",
    "m_count.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 39),\n",
       " ('woman', 26),\n",
       " ('hiv', 21),\n",
       " ('live', 13),\n",
       " ('people', 11),\n",
       " ('new', 8),\n",
       " ('resource', 8),\n",
       " ('movement', 8),\n",
       " ('work', 7),\n",
       " ('right', 7)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_count = Counter(t_clean)\n",
    "w_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_doc = nlp(\" \".join(t_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = [(w.text, w.pos_) for w in clean_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = Counter([p[1] for p in pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'VERB': 162,\n",
       "         'ADJ': 163,\n",
       "         'NOUN': 491,\n",
       "         'ADV': 30,\n",
       "         'INTJ': 7,\n",
       "         'X': 9,\n",
       "         'NUM': 1,\n",
       "         'PRON': 7,\n",
       "         'PROPN': 3,\n",
       "         'AUX': 3,\n",
       "         'ADP': 2,\n",
       "         'DET': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data from high posters to include interaction stats, clean up data a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fol = os.path.join(data.__path__[0],\"raw\")\n",
    "\n",
    "jsons = glob2.glob(os.path.join(data_fol, \"*30day*\"))\n",
    "\n",
    "tweets_dict = {}\n",
    "for n, file in enumerate(jsons):\n",
    "    with open(file) as f:\n",
    "        j = json.load(f)\n",
    "        for k in j.keys():\n",
    "            key = \"{}{}\".format(n, k)\n",
    "            tweets_dict[key] = j[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "qu = []\n",
    "for key in tweets_dict.keys():\n",
    "    if 'query' in key:\n",
    "        qu.append(key)\n",
    "        \n",
    "for k in qu:\n",
    "    del tweets_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'text', 'source', 'truncated', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'retweeted_status', 'is_quote_status', 'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'entities', 'favorited', 'retweeted', 'filter_level', 'lang', 'matching_rules'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_dict['00'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.DataFrame(index=range(len(tweets_dict.keys())))\n",
    "\n",
    "for n, key in enumerate(tweets_dict.keys()):\n",
    "    tweets_df.at[n, 't_id'] = tweets_dict[key]['id']\n",
    "    tweets_df.at[n, 'u_id'] = tweets_dict[key]['user']['id']\n",
    "    tweets_df.at[n, 'u_name'] = tweets_dict[key]['user']['screen_name']\n",
    "    tweets_df.at[n, 'n_followers'] = tweets_dict[key]['user']['followers_count']\n",
    "    tweets_df.at[n, 'verified'] = tweets_dict[key]['user']['verified']\n",
    "    tweets_df.at[n, 'datetime'] = tweets_dict[key]['created_at']\n",
    "    tweets_df.at[n, 'n_replies'] = tweets_dict[key]['reply_count']\n",
    "    tweets_df.at[n, 'n_rts'] = tweets_dict[key]['retweet_count']\n",
    "    tweets_df.at[n, 'n_faves'] = tweets_dict[key]['favorite_count']\n",
    "    tweets_df.at[n, 'text'] = tweets_dict[key]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>u_id</th>\n",
       "      <th>u_name</th>\n",
       "      <th>n_followers</th>\n",
       "      <th>verified</th>\n",
       "      <th>datetime</th>\n",
       "      <th>n_replies</th>\n",
       "      <th>n_rts</th>\n",
       "      <th>n_faves</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.360015e+18</td>\n",
       "      <td>6.046511e+08</td>\n",
       "      <td>CindersJj</td>\n",
       "      <td>740.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Thu Feb 11 23:56:41 +0000 2021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RT @EarlOfSidmouth: Everyone whoâ€™s ended up wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.360015e+18</td>\n",
       "      <td>1.028001e+18</td>\n",
       "      <td>channelchek</td>\n",
       "      <td>1113.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Thu Feb 11 23:56:40 +0000 2021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Artificial Intelligence used in machine learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.360014e+18</td>\n",
       "      <td>1.086462e+18</td>\n",
       "      <td>retrovi_fighter</td>\n",
       "      <td>344.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Thu Feb 11 23:54:27 +0000 2021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RT @CDC_HIVAIDS: CDC's updated fact sheet summ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.360014e+18</td>\n",
       "      <td>2.651354e+09</td>\n",
       "      <td>ltmatthews</td>\n",
       "      <td>443.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Thu Feb 11 23:54:04 +0000 2021</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Congratulations ðŸŽ‰ @pchitneni on her @HarvardCF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.360013e+18</td>\n",
       "      <td>2.167503e+08</td>\n",
       "      <td>Jpofgwynedd</td>\n",
       "      <td>5323.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Thu Feb 11 23:50:04 +0000 2021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RT @EarlOfSidmouth: Everyone whoâ€™s ended up wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           t_id          u_id           u_name  n_followers verified  \\\n",
       "0  1.360015e+18  6.046511e+08        CindersJj        740.0    False   \n",
       "1  1.360015e+18  1.028001e+18      channelchek       1113.0    False   \n",
       "2  1.360014e+18  1.086462e+18  retrovi_fighter        344.0    False   \n",
       "3  1.360014e+18  2.651354e+09       ltmatthews        443.0    False   \n",
       "4  1.360013e+18  2.167503e+08      Jpofgwynedd       5323.0    False   \n",
       "\n",
       "                         datetime  n_replies  n_rts  n_faves  \\\n",
       "0  Thu Feb 11 23:56:41 +0000 2021        0.0    0.0      0.0   \n",
       "1  Thu Feb 11 23:56:40 +0000 2021        0.0    0.0      0.0   \n",
       "2  Thu Feb 11 23:54:27 +0000 2021        0.0    0.0      0.0   \n",
       "3  Thu Feb 11 23:54:04 +0000 2021        2.0    1.0     22.0   \n",
       "4  Thu Feb 11 23:50:04 +0000 2021        0.0    0.0      0.0   \n",
       "\n",
       "                                                text  \n",
       "0  RT @EarlOfSidmouth: Everyone whoâ€™s ended up wi...  \n",
       "1  Artificial Intelligence used in machine learni...  \n",
       "2  RT @CDC_HIVAIDS: CDC's updated fact sheet summ...  \n",
       "3  Congratulations ðŸŽ‰ @pchitneni on her @HarvardCF...  \n",
       "4  RT @EarlOfSidmouth: Everyone whoâ€™s ended up wi...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   t_id         2000 non-null   float64\n",
      " 1   u_id         2000 non-null   float64\n",
      " 2   u_name       2000 non-null   object \n",
      " 3   n_followers  2000 non-null   float64\n",
      " 4   verified     2000 non-null   object \n",
      " 5   datetime     2000 non-null   object \n",
      " 6   n_replies    2000 non-null   float64\n",
      " 7   n_rts        2000 non-null   float64\n",
      " 8   n_faves      2000 non-null   float64\n",
      " 9   text         2000 non-null   object \n",
      "dtypes: float64(6), object(4)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['n_followers', 'n_replies', 'n_rts', 'n_faves',]\n",
    "for col in num_cols:\n",
    "    tweets_df[col] = tweets_df[col].astype(int)\n",
    "\n",
    "for col in [ 't_id', 'u_id']:\n",
    "    tweets_df[col] = tweets_df[col].astype(float)\n",
    "    \n",
    "tweets_df['verified'] = tweets_df['verified'].astype('bool')\n",
    "tweets_df['datetime'] = pd.to_datetime(tweets_df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype              \n",
      "---  ------       --------------  -----              \n",
      " 0   t_id         2000 non-null   float64            \n",
      " 1   u_id         2000 non-null   float64            \n",
      " 2   u_name       2000 non-null   object             \n",
      " 3   n_followers  2000 non-null   int32              \n",
      " 4   verified     2000 non-null   bool               \n",
      " 5   datetime     2000 non-null   datetime64[ns, UTC]\n",
      " 6   n_replies    2000 non-null   int32              \n",
      " 7   n_rts        2000 non-null   int32              \n",
      " 8   n_faves      2000 non-null   int32              \n",
      " 9   text         2000 non-null   object             \n",
      "dtypes: bool(1), datetime64[ns, UTC](1), float64(2), int32(4), object(2)\n",
      "memory usage: 111.5+ KB\n"
     ]
    }
   ],
   "source": [
    "tweets_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.drop_duplicates(subset=['t_id', 'text'], keep='first', inplace=True, ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort data by different interaction types and find common terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_id</th>\n",
       "      <th>u_id</th>\n",
       "      <th>u_name</th>\n",
       "      <th>n_followers</th>\n",
       "      <th>verified</th>\n",
       "      <th>datetime</th>\n",
       "      <th>n_replies</th>\n",
       "      <th>n_rts</th>\n",
       "      <th>n_faves</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>1.359248e+18</td>\n",
       "      <td>1.300349e+08</td>\n",
       "      <td>DrRanj</td>\n",
       "      <td>142444</td>\n",
       "      <td>True</td>\n",
       "      <td>2021-02-09 21:09:37+00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>69</td>\n",
       "      <td>1675</td>\n",
       "      <td>WOW! Our @thismorning item on #HIV testing tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1.359996e+18</td>\n",
       "      <td>9.122911e+07</td>\n",
       "      <td>EarlOfSidmouth</td>\n",
       "      <td>2261</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-11 22:40:06+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>62</td>\n",
       "      <td>Everyone whoâ€™s ended up with #hiv has a story ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>1.359563e+18</td>\n",
       "      <td>6.115853e+08</td>\n",
       "      <td>Bonn1eGreer</td>\n",
       "      <td>97419</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 18:00:41+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>Know anybody with #HIV?\\nI did...a lot of peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1447</th>\n",
       "      <td>1.359563e+18</td>\n",
       "      <td>6.115853e+08</td>\n",
       "      <td>Bonn1eGreer</td>\n",
       "      <td>97418</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-10 18:00:41+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>Know anybody with #HIV?\\nI did...a lot of peop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>1.359954e+18</td>\n",
       "      <td>6.039860e+08</td>\n",
       "      <td>sexhistorian</td>\n",
       "      <td>2966</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-11 19:52:51+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>For the next 3 years I'm researching #HIV-affe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>1.359162e+18</td>\n",
       "      <td>2.642924e+08</td>\n",
       "      <td>sbrentsimpson</td>\n",
       "      <td>3180</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-09 15:26:07+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @TashaBrooklynNY: Thank you, @theaidsmemori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>1.359161e+18</td>\n",
       "      <td>8.090747e+08</td>\n",
       "      <td>ColumbiaSIG</td>\n",
       "      <td>762</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-09 15:25:11+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @DrClauStoicescu: ðŸ“¢ In a new paper, we foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1.359161e+18</td>\n",
       "      <td>1.478306e+09</td>\n",
       "      <td>Edinenno</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-09 15:24:15+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @CDC_HIVAIDS: A CDC study found very low ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>1.359161e+18</td>\n",
       "      <td>9.851388e+17</td>\n",
       "      <td>watashi_NoUso</td>\n",
       "      <td>2148</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-09 15:24:12+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @CDC_HIVAIDS: A CDC study found very low ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.359161e+18</td>\n",
       "      <td>2.835989e+08</td>\n",
       "      <td>LeviJones1979</td>\n",
       "      <td>160</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-02-09 15:22:46+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Also if masks are so important for #COVID19 sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              t_id          u_id          u_name  n_followers  verified  \\\n",
       "1640  1.359248e+18  1.300349e+08          DrRanj       142444      True   \n",
       "62    1.359996e+18  9.122911e+07  EarlOfSidmouth         2261     False   \n",
       "947   1.359563e+18  6.115853e+08     Bonn1eGreer        97419     False   \n",
       "1447  1.359563e+18  6.115853e+08     Bonn1eGreer        97418     False   \n",
       "289   1.359954e+18  6.039860e+08    sexhistorian         2966     False   \n",
       "...            ...           ...             ...          ...       ...   \n",
       "1993  1.359162e+18  2.642924e+08   sbrentsimpson         3180     False   \n",
       "1994  1.359161e+18  8.090747e+08     ColumbiaSIG          762     False   \n",
       "1995  1.359161e+18  1.478306e+09        Edinenno           11     False   \n",
       "1996  1.359161e+18  9.851388e+17   watashi_NoUso         2148     False   \n",
       "1997  1.359161e+18  2.835989e+08   LeviJones1979          160     False   \n",
       "\n",
       "                      datetime  n_replies  n_rts  n_faves  \\\n",
       "1640 2021-02-09 21:09:37+00:00         15     69     1675   \n",
       "62   2021-02-11 22:40:06+00:00          7     17       62   \n",
       "947  2021-02-10 18:00:41+00:00          5     21       62   \n",
       "1447 2021-02-10 18:00:41+00:00          5     21       62   \n",
       "289  2021-02-11 19:52:51+00:00          5      1       36   \n",
       "...                        ...        ...    ...      ...   \n",
       "1993 2021-02-09 15:26:07+00:00          0      0        0   \n",
       "1994 2021-02-09 15:25:11+00:00          0      0        0   \n",
       "1995 2021-02-09 15:24:15+00:00          0      0        0   \n",
       "1996 2021-02-09 15:24:12+00:00          0      0        0   \n",
       "1997 2021-02-09 15:22:46+00:00          0      0        0   \n",
       "\n",
       "                                                   text  \n",
       "1640  WOW! Our @thismorning item on #HIV testing tod...  \n",
       "62    Everyone whoâ€™s ended up with #hiv has a story ...  \n",
       "947   Know anybody with #HIV?\\nI did...a lot of peop...  \n",
       "1447  Know anybody with #HIV?\\nI did...a lot of peop...  \n",
       "289   For the next 3 years I'm researching #HIV-affe...  \n",
       "...                                                 ...  \n",
       "1993  RT @TashaBrooklynNY: Thank you, @theaidsmemori...  \n",
       "1994  RT @DrClauStoicescu: ðŸ“¢ In a new paper, we foun...  \n",
       "1995  RT @CDC_HIVAIDS: A CDC study found very low ra...  \n",
       "1996  RT @CDC_HIVAIDS: A CDC study found very low ra...  \n",
       "1997  Also if masks are so important for #COVID19 sh...  \n",
       "\n",
       "[2000 rows x 10 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.sort_values(['n_replies', 'n_rts', 'n_faves'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_10_faves = tweets_df.sort_values(['n_faves', 'n_rts',  'n_replies', ], ascending=False)[:10]\n",
    "top_10_rt = tweets_df.sort_values(['n_rts',  'n_replies', 'n_faves', ], ascending=False)[:10]\n",
    "top_10_replies = tweets_df.sort_values(['n_replies', 'n_faves', 'n_rts', ], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_u_names = np.append(np.append(top_10_faves.u_name.values, top_10_rt.u_name.values), top_10_replies.u_name.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sexhistorian', 6),\n",
       " ('DrRanj', 3),\n",
       " ('Bonn1eGreer', 3),\n",
       " ('EarlOfSidmouth', 3),\n",
       " ('Winnie_Byanyima', 3),\n",
       " ('Debbie_abrahams', 2),\n",
       " ('BR999', 2),\n",
       " ('positivevibesza', 1),\n",
       " ('AccidentalHiv', 1),\n",
       " ('CedawPT', 1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(top_u_names).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorted by faves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "faves_str = \" \".join(top_10_faves.text.values)\n",
    "faves_doc, faves_clean, faves_hashtags,faves_mentions = preprocess_tweets(faves_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hiv', 6),\n",
       " ('itsasin', 1),\n",
       " ('smallpox', 1),\n",
       " ('polio', 1),\n",
       " ('womeninscience', 1),\n",
       " ('hivhometest', 1),\n",
       " ('hivtestweek', 1),\n",
       " ('uequalsu', 1)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(faves_hashtags).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorted by retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_str = \" \".join(top_10_rt.text.values)\n",
    "rt_doc, rt_clean, rt_hashtags,rt_mentions = preprocess_tweets(rt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hiv', 6),\n",
       " ('womeninscience', 2),\n",
       " ('itsasin', 1),\n",
       " ('smallpox', 1),\n",
       " ('polio', 1),\n",
       " ('sdg', 1),\n",
       " ('cedaw', 1),\n",
       " ('sexualhealth', 1),\n",
       " ('openaccess', 1),\n",
       " ('freeaccess', 1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(rt_hashtags).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorted by replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_str = \" \".join(top_10_replies.text.values)\n",
    "replies_doc, replies_clean, replies_hashtags,replies_mentions = preprocess_tweets(replies_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hiv', 5),\n",
       " ('smallpox', 1),\n",
       " ('polio', 1),\n",
       " ('itsasin', 1),\n",
       " ('hivhometest', 1),\n",
       " ('hivtestweek', 1),\n",
       " ('tellmeaboutit', 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(replies_hashtags).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### common terms from full doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hiv', 77),\n",
       " ('plhiv', 16),\n",
       " ('endhivstigma', 14),\n",
       " ('celebrateblackwomen', 13),\n",
       " ('trans', 9),\n",
       " ('nbhaad', 8),\n",
       " ('uequalsu', 7),\n",
       " ('vaccinatethemostvulnerable', 6),\n",
       " ('researchmatters', 5),\n",
       " ('biotech', 5)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all common hashtags if they show up more than once in the full tweet doc\n",
    "common_ht_all = [key for key in h_count.keys() if h_count[key] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab all hashtags that showed up more than once in the top engagement lists\n",
    "all_top_hashtags = rt_hashtags + replies_hashtags + faves_hashtags\n",
    "common_ht_top = [key for key in all_top_ht_count.keys() if all_top_ht_count[key] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adherence',\n",
       " 'aids',\n",
       " 'alcohol',\n",
       " 'biotech',\n",
       " 'celebrateblackwomen',\n",
       " 'covid19',\n",
       " 'disclose',\n",
       " 'endaids',\n",
       " 'endhivepidemic',\n",
       " 'endhivstigma',\n",
       " 'hiv',\n",
       " 'hivhometest',\n",
       " 'hivstigma',\n",
       " 'hivtestweek',\n",
       " 'hivtreatment',\n",
       " 'itsasin',\n",
       " 'lgbt',\n",
       " 'nbhaad',\n",
       " 'opioid',\n",
       " 'plhiv',\n",
       " 'plwhiv',\n",
       " 'polio',\n",
       " 'pr',\n",
       " 'prep',\n",
       " 'researchmatters',\n",
       " 'sciencenotstigma',\n",
       " 'smallpox',\n",
       " 'stigma',\n",
       " 'tbt',\n",
       " 'trans',\n",
       " 'transwomen',\n",
       " 'uequalsu',\n",
       " 'uniquelyandunapologeticallyblack',\n",
       " 'vaccinatethemostvulnerable',\n",
       " 'vaccinatethemostvulneranle',\n",
       " 'valentines',\n",
       " 'virology',\n",
       " 'women',\n",
       " 'womeninscience'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the overlap between them\n",
    "common_ht = set(common_ht_all + common_ht_top)\n",
    "common_ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually drop tags that are too generic or relate more strongly to other topics\n",
    "ht_to_drop = ['adherence', 'alcohol', 'biotech', 'celebrateblackwomen', 'covid19', 'itsasin', 'lgbt', 'opioid',\n",
    "             'pr', 'polio', 'researchmatters', 'smallpox', 'tbt', 'trans', 'transwomen', 'uniquelyandunapologeticallyblack', \n",
    "             'valentines', 'virology', 'women', 'womeninscience', 'nbhaad', 'stigma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ht in ht_to_drop:\n",
    "    try:\n",
    "        common_ht.remove(ht)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aids',\n",
       " 'disclose',\n",
       " 'endaids',\n",
       " 'endhivepidemic',\n",
       " 'endhivstigma',\n",
       " 'hiv',\n",
       " 'hivhometest',\n",
       " 'hivstigma',\n",
       " 'hivtestweek',\n",
       " 'hivtreatment',\n",
       " 'plhiv',\n",
       " 'plwhiv',\n",
       " 'prep',\n",
       " 'sciencenotstigma',\n",
       " 'uequalsu',\n",
       " 'vaccinatethemostvulnerable',\n",
       " 'vaccinatethemostvulneranle'}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save common hashtags to pickle\n",
    "pd.to_pickle(common_ht, os.path.join(data.__path__[0], r\"compiled/common_hashtags.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
