{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process in this notebook, as well as many of the functions, were taken/adapted from Derek Jedamski's course on LinkedIn Learning found here: https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tid</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1333476068192366593</th>\n",
       "      <td>Teen pregnancy is high, HIV infection rate is growing fastest among teens, and teens are experim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364161232270487553</th>\n",
       "      <td>Even though it was a charity.  Stevens thought that speaking to DHSS rather than those suffering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364161201291153414</th>\n",
       "      <td>SCORA\\nStanding Committee on Sexual &amp;amp; Reproductive Health and Rights including HIV&amp;amp;AIDS\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364161184505737217</th>\n",
       "      <td>many females are HIV+, we wish you well, blessings, we can't heal you totally, but providing rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363439109948149760</th>\n",
       "      <td>@TheRustler83 Yep. Imagine if the government were demanding HIV tests weekly in every high school.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                               full_text\n",
       "tid                                                                                                                     \n",
       "1333476068192366593  Teen pregnancy is high, HIV infection rate is growing fastest among teens, and teens are experim...\n",
       "1364161232270487553  Even though it was a charity.  Stevens thought that speaking to DHSS rather than those suffering...\n",
       "1364161201291153414  SCORA\\nStanding Committee on Sexual &amp; Reproductive Health and Rights including HIV&amp;AIDS\\...\n",
       "1364161184505737217  many females are HIV+, we wish you well, blessings, we can't heal you totally, but providing rel...\n",
       "1363439109948149760   @TheRustler83 Yep. Imagine if the government were demanding HIV tests weekly in every high school."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"../data/compiled/full_dataset_cleaned.pkl\")\n",
    "tdf = df.loc[idx[:,],['full_text']]\n",
    "tdf = tdf.reset_index().drop(columns='uid').set_index('tid')\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_things(list_to_clean, to_remove):\n",
    "    new_list = [thing for thing in list_to_clean if thing not in to_remove]\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"code adpated from LinkedIn Learning class NLP with Python for Machine Learning Essential Training by Derek Jedamski\"\"\"\n",
    "    \n",
    "    text = \"\".join([word for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]   \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_ngrams(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = \" \".join([ps.stem(word) for word in tokens if word not in stopwords])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tdf['cleaned'] = tdf['full_text'].apply(lambda x: clean_text(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer=clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(tdf['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7662, 32606)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X.toarray())\n",
    "X_df.columns = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>000</th>\n",
       "      <th>0008</th>\n",
       "      <th>001tea</th>\n",
       "      <th>008WORLD</th>\n",
       "      <th>00s</th>\n",
       "      <th>010</th>\n",
       "      <th>0121</th>\n",
       "      <th>01482</th>\n",
       "      <th>...</th>\n",
       "      <th>デュエマ</th>\n",
       "      <th>养皋ｄ</th>\n",
       "      <th>ｐｒ</th>\n",
       "      <th>ﾉ</th>\n",
       "      <th>𝐐𝐔𝐄𝐄𝐍ᵇᵍº</th>\n",
       "      <th>𝑱𝒐𝒌𝒆𝒔</th>\n",
       "      <th>𝗡𝗢𝗪</th>\n",
       "      <th>𝗥𝗘𝗚𝗜𝗦𝗧𝗘𝗥</th>\n",
       "      <th>𝘽𝙍𝙀𝘼𝙆𝙄𝙉𝙂</th>\n",
       "      <th>𝙊𝙏𝘾</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32606 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  000  0008  001tea  008WORLD  00s  010  0121  01482  ...  デュエマ  养皋ｄ  \\\n",
       "0  0  0    0     0       0         0    0    0     0      0  ...     0    0   \n",
       "1  0  0    0     0       0         0    0    0     0      0  ...     0    0   \n",
       "2  0  0    0     0       0         0    0    0     0      0  ...     0    0   \n",
       "3  0  0    0     0       0         0    0    0     0      0  ...     0    0   \n",
       "4  0  0    0     0       0         0    0    0     0      0  ...     0    0   \n",
       "\n",
       "   ｐｒ  ﾉ  𝐐𝐔𝐄𝐄𝐍ᵇᵍº  𝑱𝒐𝒌𝒆𝒔  𝗡𝗢𝗪  𝗥𝗘𝗚𝗜𝗦𝗧𝗘𝗥  𝘽𝙍𝙀𝘼𝙆𝙄𝙉𝙂  𝙊𝙏𝘾  \n",
       "0   0  0         0      0    0         0         0    0  \n",
       "1   0  0         0      0    0         0         0    0  \n",
       "2   0  0         0      0    0         0         0    0  \n",
       "3   0  0         0      0    0         0         0    0  \n",
       "4   0  0         0      0    0         0         0    0  \n",
       "\n",
       "[5 rows x 32606 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_vect = CountVectorizer(ngram_range=(2,2))\n",
    "X_counts = ngram_vect.fit_transform(data['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf['cleaned'] = tdf['full_text'].apply(lambda x: clean_text(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1333476068192366593</th>\n",
       "      <td>Teen pregnancy is high, HIV infection rate is growing fastest among teens, and teens are experim...</td>\n",
       "      <td>[teen, pregnancy, high, hiv, infection, rate, growing, fastest, among, teen, teen, experimenting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364161232270487553</th>\n",
       "      <td>Even though it was a charity.  Stevens thought that speaking to DHSS rather than those suffering...</td>\n",
       "      <td>[even, though, charity, stevens, thought, speaking, dhss, rather, suffering, 1980, 1990, hiv, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364161201291153414</th>\n",
       "      <td>SCORA\\nStanding Committee on Sexual &amp;amp; Reproductive Health and Rights including HIV&amp;amp;AIDS\\...</td>\n",
       "      <td>[scora, standing, committee, sexual, amp, reproductive, health, right, including, hivampaids, ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364161184505737217</th>\n",
       "      <td>many females are HIV+, we wish you well, blessings, we can't heal you totally, but providing rel...</td>\n",
       "      <td>[many, female, hiv, wish, well, blessing, cant, heal, totally, providing, relief, medicine, salv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363439109948149760</th>\n",
       "      <td>@TheRustler83 Yep. Imagine if the government were demanding HIV tests weekly in every high school.</td>\n",
       "      <td>[therustler83, yep, imagine, government, demanding, hiv, test, weekly, every, high, school]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                               full_text  \\\n",
       "tid                                                                                                                        \n",
       "1333476068192366593  Teen pregnancy is high, HIV infection rate is growing fastest among teens, and teens are experim...   \n",
       "1364161232270487553  Even though it was a charity.  Stevens thought that speaking to DHSS rather than those suffering...   \n",
       "1364161201291153414  SCORA\\nStanding Committee on Sexual &amp; Reproductive Health and Rights including HIV&amp;AIDS\\...   \n",
       "1364161184505737217  many females are HIV+, we wish you well, blessings, we can't heal you totally, but providing rel...   \n",
       "1363439109948149760   @TheRustler83 Yep. Imagine if the government were demanding HIV tests weekly in every high school.   \n",
       "\n",
       "                                                                                                                 cleaned  \n",
       "tid                                                                                                                       \n",
       "1333476068192366593  [teen, pregnancy, high, hiv, infection, rate, growing, fastest, among, teen, teen, experimenting...  \n",
       "1364161232270487553  [even, though, charity, stevens, thought, speaking, dhss, rather, suffering, 1980, 1990, hiv, ha...  \n",
       "1364161201291153414  [scora, standing, committee, sexual, amp, reproductive, health, right, including, hivampaids, ci...  \n",
       "1364161184505737217  [many, female, hiv, wish, well, blessing, cant, heal, totally, providing, relief, medicine, salv...  \n",
       "1363439109948149760          [therustler83, yep, imagine, government, demanding, hiv, test, weekly, every, high, school]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf = tfidf_vect.fit_transform(tdf['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>000</th>\n",
       "      <th>0008</th>\n",
       "      <th>001tea</th>\n",
       "      <th>008WORLD</th>\n",
       "      <th>00s</th>\n",
       "      <th>010</th>\n",
       "      <th>0121</th>\n",
       "      <th>01482</th>\n",
       "      <th>...</th>\n",
       "      <th>デュエマ</th>\n",
       "      <th>养皋ｄ</th>\n",
       "      <th>ｐｒ</th>\n",
       "      <th>ﾉ</th>\n",
       "      <th>𝐐𝐔𝐄𝐄𝐍ᵇᵍº</th>\n",
       "      <th>𝑱𝒐𝒌𝒆𝒔</th>\n",
       "      <th>𝗡𝗢𝗪</th>\n",
       "      <th>𝗥𝗘𝗚𝗜𝗦𝗧𝗘𝗥</th>\n",
       "      <th>𝘽𝙍𝙀𝘼𝙆𝙄𝙉𝙂</th>\n",
       "      <th>𝙊𝙏𝘾</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32606 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0  000  0008  001tea  008WORLD  00s  010  0121  01482  ...  デュエマ  \\\n",
       "0  0.0  0.0  0.0   0.0     0.0       0.0  0.0  0.0   0.0    0.0  ...   0.0   \n",
       "1  0.0  0.0  0.0   0.0     0.0       0.0  0.0  0.0   0.0    0.0  ...   0.0   \n",
       "2  0.0  0.0  0.0   0.0     0.0       0.0  0.0  0.0   0.0    0.0  ...   0.0   \n",
       "3  0.0  0.0  0.0   0.0     0.0       0.0  0.0  0.0   0.0    0.0  ...   0.0   \n",
       "4  0.0  0.0  0.0   0.0     0.0       0.0  0.0  0.0   0.0    0.0  ...   0.0   \n",
       "\n",
       "   养皋ｄ   ｐｒ    ﾉ  𝐐𝐔𝐄𝐄𝐍ᵇᵍº  𝑱𝒐𝒌𝒆𝒔  𝗡𝗢𝗪  𝗥𝗘𝗚𝗜𝗦𝗧𝗘𝗥  𝘽𝙍𝙀𝘼𝙆𝙄𝙉𝙂  𝙊𝙏𝘾  \n",
       "0  0.0  0.0  0.0       0.0    0.0  0.0       0.0       0.0  0.0  \n",
       "1  0.0  0.0  0.0       0.0    0.0  0.0       0.0       0.0  0.0  \n",
       "2  0.0  0.0  0.0       0.0    0.0  0.0       0.0       0.0  0.0  \n",
       "3  0.0  0.0  0.0       0.0    0.0  0.0       0.0       0.0  0.0  \n",
       "4  0.0  0.0  0.0       0.0    0.0  0.0       0.0       0.0  0.0  \n",
       "\n",
       "[5 rows x 32606 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns = tfidf_vect.get_feature_names()\n",
    "X_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
